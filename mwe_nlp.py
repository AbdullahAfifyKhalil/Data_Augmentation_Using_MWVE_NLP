# -*- coding: utf-8 -*-
"""MWE_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zO3GnSmxUUyViMl4EPrmIlrI_8GF44Jn
"""

from google.colab import drive
drive.mount('/content/drive')
!git clone https://gitlab.com/parseme/parseme_corpus_ar.git



!pip install conllu
!pip install stanza
!pip install fuzzywuzzy
!pip install fuzzysearch
!pip install pyarabic
!pip install farasa
!pip install arabic_reshaper
!pip install python-bidi
!pip install rapidfuzz

import pandas as pd
from conllu import parse
import fuzzysearch
import pyarabic
import farasa
import nltk
import os
import re
#from farasa.segmenter import FarasaSegmenter
#from farasa.diacratizer import FarasaDiacritizer
#from farasa.stemmer import FarasaStemmer

nltk.download('punkt')

#- first dataset as tokens

with open("/content/parseme_corpus_ar/train.cupt", "r", encoding="utf-8") as f:
    data = f.read()

sentences = parse(data)

tokens = []
for sentence in sentences:
    for token in sentence:
        token_dict = {}
        for key, value in token.items():
            token_dict[key] = value
        tokens.append(token_dict)

df1 = pd.DataFrame(tokens)

df1.head(2)

# this means the MWE -->  MWE with ID = 1 , type = LVC.full (light verb construction & the light verb and its complement are both fully realized in the sentence)
print(df1['parseme:mwe'][1])
print(df1['misc'][1])

#- first dataset as sntences

with open("/content/parseme_corpus_ar/train.cupt", "r", encoding="utf-8") as f:
    data = f.read()

sentences = parse(data)
tokens = []
sentence_dicts = []
for sentence in sentences:
  for token in sentence:
    token_dict = {}
    for key, value in token.items():
      token_dict[key] = value
    tokens.append(token_dict)
    sentence_dict = {}
    for key, value in sentence.metadata.items():
        sentence_dict[key] = value
    sentence_dict['tokens'] = sentence
    sentence_dicts.append(sentence_dict)

df_sent = pd.DataFrame(sentence_dicts)

df_sent.drop_duplicates(subset=['text'], inplace=True)

df_sent.describe()

df_sent.head(20)

print(df_sent['tokens'][0][1]['parseme:mwe'])
print(df_sent['tokens'][0][1]['lemma'])



#- the second dataset as sentences

path = '/content/drive/MyDrive/the_new_dataset'
sentences = []

for dirpath, dirnames, filenames in os.walk(path):
    for filename in filenames:
        if filename.endswith('.txt'):
            with open(os.path.join(dirpath, filename), 'r', encoding='utf-8') as f:
                text = f.read()

            sentence_list = nltk.sent_tokenize(text)

            sentences.extend(sentence_list)

df2 = pd.DataFrame({'sentences': sentences})

df2.head(3)

import nltk
from nltk.stem.isri import ISRIStemmer

def tokenize_and_lemmatize(text):
    # tokenize words
    tokens = nltk.word_tokenize(text)

    # lemmatize words
    stemmer = ISRIStemmer()
    lemmas = [stemmer.stem(token) for token in tokens]

    return tokens, lemmas

df2[['tokens', 'lemmas']] = df2['sentences'].apply(lambda x: pd.Series(tokenize_and_lemmatize(x)))

print(df_sent.columns)
print(df2.columns)
print(df1.columns)

df2.head()

df2.to_csv('New_df2.csv', index=False)

unique_mwes = df1['parseme:mwe'].unique()
unique_mwes

unique_mwes = df_sent['tokens'][0]
unique_mwes

# sentence to sentence
import csv
from rapidfuzz import process, fuzz

# Open a CSV file for writing
with open('results.csv', 'w', newline='', encoding='utf-8') as csvfile:
    # Create a writer object
    writer = csv.writer(csvfile)

    # Write the header row
    writer.writerow(['sentence', 'matches'])

    # Loop over the sentences in df2
    for sentence in df_sent['text'][:5]:
        try:
            # Find the best 5 matches for the sentence in df2 with score higher than 70%
            matches = process.extract(sentence, df2['sentences'], scorer=fuzz.token_set_ratio, score_cutoff=50, limit=5)
            # Write the result to the CSV file
            writer.writerow([sentence, [match[0] for match in matches]])
        except Exception as e:
            # If an exception occurs, print an error message and continue to the next sentence
            print(f"Error processing sentence: {sentence} - {e}")
            continue

import csv
from rapidfuzz import process, fuzz


# Open a CSV file for writing
with open('results.csv', 'w', newline='', encoding='utf-8') as csvfile:
    # Create a writer object
    writer = csv.writer(csvfile)

    # Write the header row
    writer.writerow(['token', 'matches'])

    # Loop over the tokens in df_sent
    for token in df_sent['tokens']:
        try:
            # Find the best 5 matches for the token in df2 with score higher than 50%
            matches = process.extract(token, df2['sentences'], scorer=fuzz.token_set_ratio, score_cutoff=50, limit=5)
            # Write the result to the CSV file
            writer.writerow([token, [match[0] for match in matches]])
        except Exception as e:
            # If an exception occurs, print an error message and continue to the next token
            print(f"Error processing token: {token} - {e}")
            continue

# compare token to sentence

import csv
from rapidfuzz import process, fuzz

# Open a CSV file for writing
with open('results.csv', 'w', newline='', encoding='utf-8') as csvfile:
    # Create a writer object
    writer = csv.writer(csvfile)

    # Write the header row
    writer.writerow(['token', 'matches'])

    # Loop over the rows of df_sent
    for i, row in df_sent.iterrows():
        # Loop over the tokens in the 'tokens' column of the current row
        for token in row['tokens']:
            try:
                # Find the best 5 matches for the token in df2 with score higher than 50%
                matches = process.extract(str(token), df2['sentences'], scorer=fuzz.token_set_ratio, score_cutoff=50, limit=5)
                # Write the result to the CSV file
                writer.writerow([token, [match[0] for match in matches]])
            except Exception as e:
                # If an exception occurs, print an error message and continue to the next token
                print(f"Error processing token: {token} - {e}")
                continue

from rapidfuzz import process, fuzz
import csv

# Loop over the rows of df_sent
for i, row in df_sent.iloc[1:2].iterrows():
    # Open a CSV file for writing
    with open('results.csv', 'w', newline='', encoding='utf-8') as csvfile:
        # Create a writer object
        writer = csv.writer(csvfile)

        # Write the header row
        writer.writerow(['token', 'matches'])

        # Loop over the tokens in the 'tokens' column of the current row
        for token in row['tokens']:
            try:
                # Find the best 5 matches for the token in df2 with score higher than 50%
                matches = process.extract(str(token), df2['sentences'], scorer=fuzz.token_set_ratio, score_cutoff=50, limit=5)
                # Write the result to the CSV file
                writer.writerow([token, [match[0] for match in matches]])
            except Exception as e:
                # If an exception occurs, print an error message and continue to the next token
                print(f"Error processing token: {token} - {e}")
                continue

from google.colab import files
files.download('results.csv')

for sentence in df_sent['text'][:2]:
      print(sentence)

import conllu
import pandas as pd

# Load the .cupt file
with open('/content/parseme_corpus_ar/train.cupt', 'r', encoding='utf-8') as f:
    data = f.read()

# Parse the .cupt file
sentences = conllu.parse(data)

# Create an empty dataframe to store the results
df = pd.DataFrame(columns=['sentence', 'annotation', 'mwe_type'])

# Loop over each sentence
for sentence in sentences:
    # Get the sentence text
    sentence_text = sentence.metadata['text']

    # Create a list to store the tokens of each MWE
    mwe_tokens = []

    # Loop over each token in the sentence
    for token in sentence:
        # Check if the token is part of an MWE
        if token['misc'] is not None and 'SpaceAfter=No' in token['misc']:
            # The token is part of an MWE
            # Get the MWE annotation
            mwe = token['misc']['SpaceAfter=No']

            # Add the token to the list of MWE tokens
            mwe_tokens.append(token['form'])

            # If this is the last token of the MWE, add the MWE to the dataframe
            if token['misc']['SpaceAfter'] == 'No':
                mwe_annotation = ' '.join(mwe_tokens)
                df = df.append({'sentence': sentence_text,
                                'annotation': mwe_annotation,
                                'mwe_type': mwe},
                               ignore_index=True)
                # Clear the list of MWE tokens
                mwe_tokens = []
        else:
            # If the current token is not part of an MWE, add it to the dataframe
            df = df.append({'sentence': sentence_text,
                            'annotation': token['form'],
                            'mwe_type': ''},
                           ignore_index=True)


# Group the dataframe by MWE type and sentence
grouped_df = df.groupby(['mwe_type', 'sentence'])['annotation'].apply(list).reset_index()

# Print the resulting dataframe
print(grouped_df)